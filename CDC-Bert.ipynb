{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/vijjus/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  1.14.0\n",
      "Hub version:  0.7.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy, categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "\n",
    "print(\"TF version: \", tf.__version__)\n",
    "print(\"Hub version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_bert_path =  '/home/vijjus/bert/bert/'\n",
    "data_path = '/home/vijjus/datasets/'  # path to ner_dataset.csv file , from \n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "\n",
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0,local_bert_path)\n",
    "sys.path.insert(0,data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vijjus/bert/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import run_classifier_with_tfhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will limit the sentences to 64 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow hub path to BERT module of choice\n",
    "bert_url = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "# Define maximal length of input 'sentences' (post tokenization).\n",
    "max_length = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing ##\n",
    "\n",
    "BERT uses the wordpiece tokenizer. With this tokenizer, the original CDC dataset can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_url)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vijjus/bert/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vijjus/bert/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWord(word):\n",
    "    \"\"\"\n",
    "    \n",
    "    arguments: word\n",
    "    returns: dictionary with tokens and labels\n",
    "    \"\"\"\n",
    "    # the dataset contains various '\"\"\"' combinations which we choose to truncate to '\"', etc. \n",
    "    if word == '\"\"\"\"':\n",
    "        word = '\"'\n",
    "    elif word == '``':\n",
    "        word = '`'\n",
    "        \n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    tokenLength = len(tokens)      # find number of tokens corresponfing to word to later add 'X' tokens to labels\n",
    "    \n",
    "    addDict = dict()\n",
    "    \n",
    "    addDict['wordToken'] = tokens\n",
    "    addDict['tokenLength'] = tokenLength\n",
    "    \n",
    "    return addDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path + 'cdc_train_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>57 year old male with contusion to face after ...</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a 45 year old male fell on arm while working h...</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>58 year old male with cervical strain back pai...</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>33 year old male laceration to hand from a raz...</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>53 year old male at work in a warehouse doing ...</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  sex  age  \\\n",
       "0           0  57 year old male with contusion to face after ...    1   57   \n",
       "1           1  a 45 year old male fell on arm while working h...    1   45   \n",
       "2           2  58 year old male with cervical strain back pai...    1   58   \n",
       "3           3  33 year old male laceration to hand from a raz...    1   33   \n",
       "4           4  53 year old male at work in a warehouse doing ...    1   53   \n",
       "\n",
       "   event  \n",
       "0     62  \n",
       "1     42  \n",
       "2     26  \n",
       "3     60  \n",
       "4     71  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc041a22013346bdbb21605df675a431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=153956), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read the file line by line and construct sentences. A sentence end is marked by the word 'sentence' in the next row.\n",
    "You need to take care of that. Also, you need to cap sentence length using max_length. Sentences which are shorter than \n",
    "max_length need to be padded. Also, we choose to end all sentences with a [SEP] token, padded or not. \n",
    "\"\"\"\n",
    "\n",
    "# lists for sentences, tokens, labels, etc.  \n",
    "sentenceList = []\n",
    "sentenceTokenList = []\n",
    "sentLengthList = []\n",
    "\n",
    "# lists for BERT input\n",
    "bertSentenceIDs = []\n",
    "bertMasks = []\n",
    "bertSequenceIDs = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "total_len = len(df)\n",
    "\n",
    "for i in tqdm_notebook(range(total_len)):\n",
    "    \n",
    "    # always start with [CLS] tokens\n",
    "    sentenceTokens = ['[CLS]']\n",
    "    \n",
    "    text = df.iloc[i]['text'].lower()\n",
    "    \n",
    "    labels.append(int(df.iloc[i]['event']))\n",
    "    \n",
    "    words = text.split(\" \")\n",
    "    \n",
    "    for word in words:\n",
    "        addDict = addWord(word)\n",
    "        sentenceTokens += addDict['wordToken']\n",
    "    sentenceTokens += ['[SEP]'] + ['[PAD]'] * (max_length -1 - len(sentenceTokens))\n",
    "    \n",
    "    sentenceList.append(text)\n",
    "    sentenceTokenList.append(sentenceTokens)\n",
    "    sentLengthList.append(len(sentenceTokens))\n",
    "\n",
    "    sentenceLength = min(max_length - 1, len(sentenceTokens))\n",
    "    bertSentenceIDs.append(tokenizer.convert_tokens_to_ids(sentenceTokens))\n",
    "    bertMasks.append([1] * (sentenceLength + 1) + [0] * (max_length - 1 - sentenceLength))\n",
    "    bertSequenceIDs.append([0] * (max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'58 year old male with cervical strain back pain status post restrained taxi driver in low speed rear end mvc no loss of consciousness no ab deployed'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceList[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153956"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.9370e+03, 5.4920e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        4.8000e+01, 5.0000e+01, 1.2990e+03, 2.7880e+03, 8.4400e+02,\n",
       "        2.0000e+00, 1.2320e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 1.5620e+03, 2.2173e+04, 3.8600e+02, 0.0000e+00,\n",
       "        0.0000e+00, 5.2000e+01, 9.7000e+02, 3.9070e+03, 1.1678e+04,\n",
       "        0.0000e+00, 8.9860e+03, 5.2000e+01, 3.3460e+04, 4.4290e+03,\n",
       "        2.9330e+03, 9.6000e+01, 3.1225e+04, 9.0780e+03, 4.0000e+00,\n",
       "        0.0000e+00, 8.8800e+02, 1.2000e+01, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 1.3730e+03]),\n",
       " array([10.        , 11.85416667, 13.70833333, 15.5625    , 17.41666667,\n",
       "        19.27083333, 21.125     , 22.97916667, 24.83333333, 26.6875    ,\n",
       "        28.54166667, 30.39583333, 32.25      , 34.10416667, 35.95833333,\n",
       "        37.8125    , 39.66666667, 41.52083333, 43.375     , 45.22916667,\n",
       "        47.08333333, 48.9375    , 50.79166667, 52.64583333, 54.5       ,\n",
       "        56.35416667, 58.20833333, 60.0625    , 61.91666667, 63.77083333,\n",
       "        65.625     , 67.47916667, 69.33333333, 71.1875    , 73.04166667,\n",
       "        74.89583333, 76.75      , 78.60416667, 80.45833333, 82.3125    ,\n",
       "        84.16666667, 86.02083333, 87.875     , 89.72916667, 91.58333333,\n",
       "        93.4375    , 95.29166667, 97.14583333, 99.        ]),\n",
       " <a list of 48 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(labels, bins=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bertSequenceIDs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 153956, 110)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSentences = len(bert_inputs[0])\n",
    "np.random.seed(0)\n",
    "training_examples = np.random.binomial(1, 0.7, numSentences)\n",
    "num_training_examples = np.sum(training_examples)\n",
    "num_test_examples = numSentences - num_training_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153956"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numSentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62, 42, 26, 60, 71, 62, 60, 69, 71, 71]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "event2id = {}\n",
    "id2event = {}\n",
    "events = sorted(set(labels))\n",
    "for i, event in enumerate(events):\n",
    "    event2id[event] = i\n",
    "    id2event[i] = event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClasses = df.event.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentence_ids = []\n",
    "trainMasks = []\n",
    "trainSequence_ids = []\n",
    "\n",
    "testSentence_ids = []\n",
    "testMasks = []\n",
    "testSequence_ids = []\n",
    "\n",
    "Labels_train_list = []\n",
    "Labels_test_list = []\n",
    "\n",
    "for example in range(numSentences):\n",
    "    if training_examples[example] == 1:\n",
    "        trainSentence_ids.append(bert_inputs[0][example])\n",
    "        trainMasks.append(bert_inputs[1][example])\n",
    "        trainSequence_ids.append(bert_inputs[2][example])\n",
    "        Labels_train_list.append(event2id[labels[example]])\n",
    "    else:\n",
    "        testSentence_ids.append(bert_inputs[0][example])\n",
    "        testMasks.append(bert_inputs[1][example])\n",
    "        testSequence_ids.append(bert_inputs[2][example])\n",
    "        Labels_test_list.append(event2id[labels[example]])\n",
    "        \n",
    "X_train = np.array([trainSentence_ids,trainMasks,trainSequence_ids])\n",
    "X_test = np.array([testSentence_ids,testMasks,testSequence_ids])\n",
    "\n",
    "Labels_train = np.zeros((num_training_examples,numClasses), dtype=np.float32)\n",
    "Labels_test = np.zeros((num_test_examples,numClasses), dtype=np.float32)\n",
    "\n",
    "for x, label in enumerate(Labels_train_list):\n",
    "    Labels_train[x][label] = 1.0\n",
    "    \n",
    "for x, label in enumerate(Labels_test_list):\n",
    "    Labels_test[x][label] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46328, 48)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(id2event[np.argmax(Labels_train[0])] == labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 107628, 110)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a parameter pair k_start, k_end to look at slices. This helps with quick tests.\n",
    "k_start = 0\n",
    "k_end = -1\n",
    "\n",
    "if k_end == -1:\n",
    "    k_end_train = X_train[0].shape[0]\n",
    "    k_end_test = X_test[0].shape[0]\n",
    "else:\n",
    "    k_end_train = k_end_test = k_end\n",
    "    \n",
    "bert_inputs_train_k = [X_train[0][k_start:k_end_train], X_train[1][k_start:k_end_train], \n",
    "                       X_train[2][k_start:k_end_train]]\n",
    "bert_inputs_test_k = [X_test[0][k_start:k_end_test], X_test[1][k_start:k_end_test], \n",
    "                      X_test[2][k_start:k_end_test]]\n",
    "\n",
    "labels_train_k = Labels_train[k_start:k_end_train]\n",
    "labels_test_k = Labels_test[k_start:k_end_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107628, 48)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46328, 48)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length + 1) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns:  cost\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    #y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    \n",
    "    #mask = (y_label < 17)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
    "\n",
    "    #y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    #y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float32)),[-1, numClasses])\n",
    "    \n",
    "    #y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
    "    \n",
    "    return tf.reduce_mean(categorical_crossentropy(y_true, y_pred, from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    y_label = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.float64)),\n",
    "                                                [-1, numClasses]), axis=1)\n",
    "    #y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    #mask = (y_label < 17)\n",
    "    #y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, numClasses]), axis=1)\n",
    "    \n",
    "    #y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted, y_label), dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_customized = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Create BERT layer, following https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
    "    init:  initialize layer. Specify various parameters regarding output types and dimensions. Very important is\n",
    "           to set the number of trainable layers.\n",
    "    build: build the layer based on parameters\n",
    "    call:  call the BERT layer within a model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_url=\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_url = bert_url\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_url, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        trainable_layers = []\n",
    "        \n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "            \n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "        #trainable_vars = [\n",
    "        #        var\n",
    "        #        for var in trainable_vars\n",
    "        #        if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "        #    ]        \n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        #result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "        #        \"pooled_output\"\n",
    "        #    ]\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdc_model(max_input_length, train_layers):\n",
    "    \"\"\"\n",
    "    Implementation of CDC model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), name=\"segment_ids\")\n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_sequence = BertLayer(n_fine_tune_layers=train_layers, pooling=\"first\")(bert_inputs)\n",
    "    \n",
    "    print(bert_sequence)\n",
    "    \n",
    "    #conv = tf.keras.layers.Conv1D(filters=192, kernel_size=(3,))(bert_sequence)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.BatchNormalization()(dense)\n",
    "    \n",
    "    do = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(48, activation='softmax', name='cdc')(do)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    losses = {\"cdc\": custom_loss}\n",
    "    lossWeights = {\"cdc\": 1.0}\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    \n",
    "    #model.compile(loss='categorical_crossentropy',\n",
    "    #              optimizer='adam',\n",
    "    #              metrics=['accuracy'])                                                \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bert_layer/bert_layer_module_apply_tokens/bert/pooler/dense/Tanh:0\", shape=(?, 768), dtype=float32)\n",
      "WARNING:tensorflow:From /home/vijjus/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vijjus/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  Tensor(\"cdc/Softmax:0\", shape=(?, 48), dtype=float32)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 110)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 110)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 110)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, 768)          108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256)          1024        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "cdc (Dense)                     (None, 48)           12336       dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,141,620\n",
      "Trainable params: 7,888,176\n",
      "Non-trainable params: 101,253,444\n",
      "__________________________________________________________________________________________________\n",
      "Train on 107628 samples, validate on 46328 samples\n",
      "WARNING:tensorflow:From /home/vijjus/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vijjus/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "107628/107628 [==============================] - 2330s 22ms/sample - loss: 1.0893 - acc: 0.6877 - val_loss: 9.4983 - val_acc: 0.1034\n",
      "Epoch 2/3\n",
      "107616/107628 [============================>.] - ETA: 0s - loss: 0.8346 - acc: 0.7516"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-63656dfa089a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_inputs_test_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"cdc\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels_test_k\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;31m#,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m#callbacks=[tensorboard]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m           \u001b[0mvalidation_in_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m           \u001b[0mprepared_feed_values_from_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    410\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Start session\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = cdc_model(max_length + 1, train_layers=1)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "model.fit(\n",
    "    bert_inputs_train_k, \n",
    "    {\"cdc\": labels_train_k },\n",
    "    validation_data=(bert_inputs_test_k, {\"cdc\": labels_test_k }),\n",
    "    epochs=3,\n",
    "    batch_size=32#,\n",
    "    #callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
